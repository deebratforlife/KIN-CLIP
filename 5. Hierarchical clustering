# Loading required packages. Install them if needed using install.packages("packagename")
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms

#import data matrix and provide a suitable name.

testing <- read.delim("C:/Users/deebratforlife/Desktop/KIN-CLIP.txt", header=FALSE, na.strings="")

# We need to transpose our data set to get the format with 1st column = Transcript names and first row = 7 features. Alternately, this can be achieved manually.
dat.n <- (t(testing))
dat.tn <- t(dat.n)

# Testing different clustering methods:

# Hierarchical clustering can be divided into two main types: agglomerative and divisive.

# Agglomerative clustering: It’s also known as AGNES (Agglomerative Nesting). 
# It works in a bottom-up manner. That is, each object is initially considered as a single-element cluster (leaf). 
# At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster (nodes). 
# This procedure is iterated until all points are member of just one single big cluster (root).
# The result is a tree which can be plotted as a dendrogram.

# Divisive hierarchical clustering: It’s also known as DIANA (Divise Analysis) and it works in a top-down manner. 
# The algorithm is an inverse order of AGNES. It begins with the root, in which all objects are included in a single cluster.
# At each step of iteration, the most heterogeneous cluster is divided into two. The process is iterated until all objects are in their own cluster.

# Note that generally agglomerative clustering is good at identifying small clusters. Divisive hierarchical clustering is good at identifying large clusters.

# First, we measure the (dis)similarity of observations using distance measures. We tested five methods:
# Euclidean | Manhattan | Pearson Correlation | Spearman Correlation | Kendall correlation distances (For details, please see the references provided in the manuscript).

# NOTE: In R, the Euclidean distance is used by default to measure the dissimilarity between each pair of observations.


# Dissimilarity matrix

# We compute and visualize the distance matrix using the functions get_dist and fviz_dist from the factoextra R package. 

# get_dist: for computing a distance matrix between the rows of a data matrix. The default distance computed is the Euclidean; 
# however, get_dist also supports distances described above.
# fviz_dist: for visualizing a distance matrix

d1 <- dist(df, method = "euclidean")
d2 <- dist(df, method = "manhattan")
d3 <- dist(df, method = "spearman")
d4 <- dist(df, method = "pearson")
d5 <- dist(df, method = "kendall")

distance <- get_dist(d1)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))

# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)

# Compute with agnes
hc2 <- agnes(df, method = "complete")

# Agglomerative coefficient
hc2$ac
## [1] 0.8531583

# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(df, method = x)$ac
}

map_dbl(m, ac)
##   average    single  complete      ward 
## 0.7379371 0.6276128 0.8531583 0.9346210

# So, ward's clustering is best..

hc3 <- agnes(df, method = "ward")
pltree(hc3, cex = 0.6, hang = -1, main = "Dendrogram of agnes") 

# compute divisive hierarchical clustering
hc4 <- diana(df)

# Divise coefficient; amount of clustering structure found
hc4$dc
## [1] 0.8514345

# plot dendrogram
pltree(hc4, cex = 0.6, hang = -1, main = "Dendrogram of diana")

# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
sub_grp <- cutree(hc5, k = 4)

# Number of members in each cluster
table(sub_grp)
## sub_grp
##  1  2  3  4 
##  7 12 19 12

# A test of clustering output using pre-built data

USArrests %>%
  mutate(cluster = sub_grp) %>%
  head
##   Murder Assault UrbanPop Rape cluster
## 1   13.2     236       58 21.2       1
## 2   10.0     263       48 44.5       2
## 3    8.1     294       80 31.0       2
## 4    8.8     190       50 19.5       3
## 5    9.0     276       91 40.6       2
## 6    7.9     204       78 38.7       2

plot(hc5, cex = 0.6)
rect.hclust(hc5, k = 4, border = 2:5)

fviz_cluster(list(data = df, cluster = sub_grp))

# Cut agnes() tree into 4 groups
hc_a <- agnes(df, method = "ward")
cutree(as.hclust(hc_a), k = 4)

# Cut diana() tree into 4 groups
hc_d <- diana(df)
cutree(as.hclust(hc_d), k = 4)

# Compute distance matrix
res.dist <- dist(df, method = "euclidean")

# Compute 2 hierarchical clusterings
hc1 <- hclust(res.dist, method = "complete")
hc2 <- hclust(res.dist, method = "ward.D2")

# Create two dendrograms
dend1 <- as.dendrogram (hc1)
dend2 <- as.dendrogram (hc2)

tanglegram(dend1, dend2)

dend_list <- dendlist(dend1, dend2)

tanglegram(dend1, dend2,
  highlight_distinct_edges = FALSE, # Turn-off dashed lines
  common_subtrees_color_lines = FALSE, # Turn-off line colors
  common_subtrees_color_branches = TRUE, # Color common branches 
  main = paste("entanglement =", round(entanglement(dend_list), 2))
  )
  
  # Determining optimal clusters using 3 methods
  
  fviz_nbclust(df, FUN = hcut, method = "wss")
  
  fviz_nbclust(df, FUN = hcut, method = "silhouette")
  
  gap_stat <- clusGap(df, FUN = hcut, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)

# Now using the actual data matrix to put it all together and combine the output with a heatmap for visualization:

d1 <- dist(dat.n,method = "euclidean", diag = FALSE, upper = FALSE)
d2 <- dist(dat.tn,method = "euclidean", diag = FALSE, upper = TRUE)

c1 <- hclust(d1, method = "ward.D2", members = NULL)
c2 <- hclust(d2, method = "ward.D2", members = NULL)

heatmap.2(dat.tn,                     # Tidy, normalised data
           Colv=as.dendrogram(c1),     # Experiments clusters in cols
           Rowv=as.dendrogram(c2),     # Protein clusters in rows
           density.info="histogram",   # Plot histogram of data and colour key
           trace="none",scale="row",               # Turn of trace lines from heat map
           col = my_palette,           # Use my colour scheme
           cexRow=0.5,cexCol=0.75)     # Amend row and column label fonts
my_palette <- colorRampPalette(c("green","red"))(n = 5)

#Variations of this basic script generates different types of heatmaps, dependent upon the data matrix identity.


# More details can be provided upon request
